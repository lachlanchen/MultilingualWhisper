import torch
import whisper
import torchaudio
from io import BytesIO
import tempfile
import os
from pprint import pprint
import json
import subprocess
import argparse
from tqdm import tqdm
import subprocess
import shutil
import os
import tempfile

# import whisper_timestamped as whisper

from lingua import Language, LanguageDetectorBuilder

import traceback



def detect_language_with_lingua(text, detector):
    """
    Detects the language of a given text using Lingua.
    Returns the ISO 639-1 code of the detected language if detection is confident; otherwise, returns None.
    """
    try:
        language = detector.detect_language_of(text)
        return language.iso_code_639_1.name.lower()  # Use .name to get the ISO code as a string
    except Exception as e:
        print(f"Language detection failed: {e}")
        return 'und'

def adjust_timestamps(speech_timestamps, audio_length=None):
    """
    Adjusts the start and end frames of speech segments to ensure continuity and completeness.
    
    :param speech_timestamps: List of dictionaries with 'start' and 'end' keys for each speech segment.
    :param audio_length: Optional. The total length of the audio. If provided, ensures the last segment ends at this time.
    """
    num_segments = len(speech_timestamps)

    if num_segments == 0:
        return  # No segments to adjust

    if num_segments == 1:
        # If only one segment, it spans the entire audio
        speech_timestamps[0]["start"] = 0
        speech_timestamps[0]["end"] = audio_length if audio_length is not None else speech_timestamps[0]["end"]
    else:
        for i in range(num_segments):
            if i == 0:
                # Set the start of the first segment to 0
                speech_timestamps[i]["start"] = 0
                # Ensure the first segment ends where the second segment starts
                speech_timestamps[i]["end"] = speech_timestamps[i + 1]["start"]
            elif i == num_segments - 1:
                # For the last segment, adjust the start to match the penultimate segment's end
                # and the end to the audio length if specified, else keep as is
                speech_timestamps[i]["start"] = speech_timestamps[i - 1]["end"]
                speech_timestamps[i]["end"] = audio_length if audio_length is not None else speech_timestamps[i]["end"]
            else:
                # For middle segments, ensure no gaps by adjusting starts and ends to match neighbors
                speech_timestamps[i]["start"] = speech_timestamps[i - 1]["end"]
                speech_timestamps[i]["end"] = speech_timestamps[i + 1]["start"]


# def clean_timestamps(speech_timestamps, audio_length):
#     """
#     Adjusts the start and end frames of speech segments to ensure consitency.
    
#     :param speech_timestamps: List of dictionaries with 'start' and 'end' keys for each speech segment.
#     :param audio_length: The total length of the audio. 
#     """
#     num_segments = len(speech_timestamps)

#     if num_segments == 0:
#         return  # No segments to adjust

#     if num_segments == 1:
#         return 
#     else:
#         for i in range(num_segments):
#             if i == 0:
#                 speech_timestamps[i]["start"] = max(speech_timestamps[i]["start"], 0)
#                 speech_timestamps[i]["end"] = min(min(speech_timestamps[i]["end"], speech_timestamps[i+1]["start"]), audio_length)
#             elif i == num_segments - 1:
               
#                 speech_timestamps[i]["start"] = max(max(speech_timestamps[i-1]["end"], speech_timestamps[i]["start"]), 0)
#                 speech_timestamps[i]["end"] = min(speech_timestamps[i]["end"], audio_length)
#             else:
#                 # For middle segments, ensure no gaps by adjusting starts and ends to match neighbors
#                 speech_timestamps[i]["start"] = max(max(speech_timestamps[i-1]["end"], speech_timestamps[i]["start"]), 0)
#                 speech_timestamps[i]["end"] = min(min(speech_timestamps[i]["end"], speech_timestamps[i+1]["start"]), audio_length)

def clean_timestamps(speech_timestamps, audio_length):
    num_segments = len(speech_timestamps)

    if num_segments == 0:
        return  # No segments to adjust

    delete_mask = []
    for i in range(num_segments):
        if speech_timestamps[i]["lang"] == "und" or speech_timestamps[i]["text"] == []:
            delete_mask.append(i)

        if i == 0:
            # Ensure the first segment starts within the audio bounds
            speech_timestamps[i]["start"] = max(speech_timestamps[i]["start"], 0)
        else:
            # Ensure the start does not precede the previous segment's end
            speech_timestamps[i]["start"] = max(speech_timestamps[i-1]["end"], speech_timestamps[i]["start"])

        if i < num_segments - 1:
            # Adjust the end to not overlap with the next segment
            speech_timestamps[i]["end"] = min(speech_timestamps[i]["end"], speech_timestamps[i+1]["start"])
        else:
            # Ensure the last segment ends within the audio bounds
            speech_timestamps[i]["end"] = min(speech_timestamps[i]["end"], audio_length)

    for i in delete_mask[::-1]:
        speech_timestamps.remove(speech_timestamps[i])



def predict_language_for_segment(audio_segment, allowed_languages=["en", "zh", "ja", "ar", "yue", "ko", "vi", "es", "fr"], model="large-v2"):
    # Convert audio segment to Mel spectrogram

    audio_segment = whisper.pad_or_trim(audio_segment)

    if model in ["large-v2"]:
        mel = whisper.log_mel_spectrogram(audio=audio_segment).to(whisper_model.device)
    else:
        mel = whisper.log_mel_spectrogram(audio=audio_segment, n_mels=128).to(whisper_model.device)

    # Detect the spoken language
    _, probs = whisper_model.detect_language(mel)

    # Filter the probabilities to only include allowed languages
    # Initialize a filtered_probs dictionary with allowed languages as keys and a default probability of 0
    filtered_probs = {lang: probs.get(lang, 0) for lang in allowed_languages}

    # pprint(filtered_probs)
    # Find the language with the highest probability among the allowed languages
    max_language = max(filtered_probs, key=filtered_probs.get)

    return max_language


# def transcribe_segment(audio_segment, start_frame, end_frame, sampling_rate, detected_language):
#     # Create a temporary file for the audio segment
#     temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
#     torchaudio.save(temp_file.name, audio_segment.unsqueeze(0), sampling_rate)
#     temp_file.close()  # Close the file so Whisper can read it
    
#     # Transcribe the audio segment using Whisper
#     try:
#         result = whisper_model.transcribe(temp_file.name, language=detected_language, word_timestamps=True)
#         transcription = result["text"]
#         segments = result["segments"]
#         pprint(result)
#         # pprint(result)
#     except Exception as e:
#         print(f"Error transcribing segment: {e}")
#         traceback.print_exc()
#         transcription = ""
#         segments = []

#         raise

#     # Clean up the temporary file
#     os.remove(temp_file.name)
#     return transcription, segments, detected_language

def transcribe_segment(audio_segment, start_frame, end_frame, sampling_rate, detected_language):

    print("start_frame: ", start_frame)
    print("end_frame: ", end_frame)
    print("sampling_rate: ", sampling_rate)

    # Create a temporary file for the audio segment
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
    torchaudio.save(temp_file.name, audio_segment.unsqueeze(0), sampling_rate)
    temp_file.close()  # Close the file so Whisper can read it
    
    # Calculate and print the audio segment length in seconds
    audio_length_seconds = (end_frame - start_frame) / sampling_rate
    print(f"Audio segment length: {audio_length_seconds:.2f} seconds")
    
    # Transcribe the audio segment using Whisper
    try:
        result = whisper_model.transcribe(temp_file.name, language=detected_language, word_timestamps=True)
        transcription = result["text"]
        segments = result["segments"]
        # pprint(result)
    except Exception as e:
        print(f"Error transcribing segment: {e}")
        traceback.print_exc()
        transcription = ""
        segments = []
        raise  # Optionally re-raise the exception if you want to handle it further up the call stack

    # Clean up the temporary file
    os.remove(temp_file.name)
    return transcription, segments, detected_language

# def update_segments_with_language(words_segments, parent_start_frame, sampling_rate, detector, detected_language=None):
#     new_segments = []

#     for segment in words_segments:
#         # segment_text = ' '.join([word['word'] for word in segment['words']])
#         segment_text = segment["text"]
#         text_language = detect_language_with_lingua(segment_text, detector)  # Use the Lingua function for language detection
        
#         # Optimize the logic for determining segment language
#         segment_language = text_language if detected_language != "yue" or text_language != "zh" else "yue"


#         # Adjust start and end frames relative to the parent segment's start frame
#         start_frame = int(segment['start'] * sampling_rate) + parent_start_frame
#         end_frame = int(segment['end'] * sampling_rate) + parent_start_frame
        
#         new_segments.append({
#             'start': start_frame,
#             'end': end_frame,
#             'lang': segment_language,
#             'text': segment_text,
#             # add words timestamps here for futhure use and also add the parent_start_frame
#         })

#     return new_segments


def filter_segments(segments):
    """
    Filter the segments to include only the keys 'start', 'end', 'lang', and 'text'.

    Parameters:
    segments (list of dicts): The original segments containing various keys.

    Returns:
    list of dicts: The filtered segments containing only the specified keys.
    """
    filtered_segments = []
    for segment in segments:
        filtered_segment = {
            'start': segment['start'],
            'end': segment['end'],
            'lang': segment['lang'],
            'text': segment['text']
        }
        filtered_segments.append(filtered_segment)
    return filtered_segments

def detect_segment_language(segment_text, detected_language, detector):
    text_language = detect_language_with_lingua(segment_text, detector)  # Language detection
        
    # Determine segment language, considering special cases
    segment_language = text_language if detected_language != "yue" or text_language != "zh" else "yue"

    return segment_language

def update_segments_with_language(words_segments, parent_start_frame, sampling_rate, detector, detected_language=None):
    new_segments = []

    for segment in words_segments:
        segment_text = segment["text"]
        segment_language = detect_segment_language(segment_text, detected_language, detector)

        # Calculate absolute start and end frames for the segment
        segment_start_frame_absolute = int(segment['start'] * sampling_rate) + parent_start_frame
        segment_end_frame_absolute = int(segment['end'] * sampling_rate) + parent_start_frame
        
        # Adjust word timestamps relative to the entire audio file
        words_with_timestamps = []
        for word in segment.get('words', []):
            word_start_absolute = int(word['start'] * sampling_rate) + parent_start_frame
            word_end_absolute = int(word['end'] * sampling_rate) + parent_start_frame
            words_with_timestamps.append({
                'word': word['word'],
                'start': word_start_absolute,
                'end': word_end_absolute,
                'probability': word.get('probability', 1.0)  # Include probability if available
            })

        new_segments.append({
            'start': segment_start_frame_absolute,
            'end': segment_end_frame_absolute,
            'lang': segment_language,
            'text': segment_text,
            'words': words_with_timestamps,  # Updated to include adjusted word timestamps
            'parent_start_frame': parent_start_frame  # Add parent_start_frame for reference
        })

    return new_segments


def frames_to_milliseconds(frames, sample_rate):
    """Converts frame numbers to milliseconds based on the given sample rate."""
    return (frames / sample_rate) * 1000

def format_timestamp(ms):
    """Converts milliseconds to 'hh:mm:ss,ms' format."""
    hours = int(ms // 3600000)
    minutes = int((ms % 3600000) // 60000)
    seconds = int((ms % 60000) // 1000)
    milliseconds = int(ms % 1000)
    return f"{hours:02}:{minutes:02}:{seconds:02},{milliseconds:03}"

def subtitles_to_srt(subtitles, sample_rate):
    """Converts subtitles to SRT format."""
    srt_content = ""
    for i, subtitle in enumerate(subtitles, start=1):
        start_ms = frames_to_milliseconds(subtitle['start'], sample_rate)
        end_ms = frames_to_milliseconds(subtitle['end'], sample_rate)
        start_srt = format_timestamp(start_ms)
        end_srt = format_timestamp(end_ms)
        srt_content += f"{i}\n{start_srt} --> {end_srt}\n{subtitle['text']}\n\n"
    return srt_content


def subtitles_to_json(subtitles, sample_rate):
    """Converts subtitles to a JSON string, formatting timestamps as 'hh:mm:ss,ms'."""
    adjusted_subtitles = []
    for subtitle in subtitles:
        adjusted_subtitle = subtitle.copy()
        # Format start and end times as 'hh:mm:ss,ms' for JSON representation
        adjusted_subtitle['start'] = format_timestamp(frames_to_milliseconds(subtitle['start'], sample_rate))
        adjusted_subtitle['end'] = format_timestamp(frames_to_milliseconds(subtitle['end'], sample_rate))
        adjusted_subtitles.append(adjusted_subtitle)
    return json.dumps(adjusted_subtitles, indent=4, ensure_ascii=False)


def save_subtitles(srt_content, json_content, srt_path, json_path):
    """Saves SRT and JSON content to specified paths."""

    print("srt path: ", srt_path)
    print("json path: ", json_path)
    with open(srt_path, "w", encoding="utf-8") as srt_file:
        srt_file.write(srt_content)

    with open(json_path, "w", encoding="utf-8") as json_file:
        json_file.write(json_content)

def refine_transcription_segments_with_punctuation(transcription_segments, sample_rate):
    """
    Refines transcription segments based on punctuation marks within the words of each segment.

    Parameters:
    - transcription_segments: List of transcription segments, each containing word-level details.
    - sample_rate: The sampling rate of the audio file.

    Returns:
    - List of refined transcription segments.
    """
    refined_segments = []
    # punctuation_marks = {'.', '?', '!', '。'}
    punctuation_marks = {
        '、',  # Adding full-width ideographic comma
        ',', '，',  # Adding half-width and full-width commas
        '.', '。',  # Adding full-width period (half-width already included)
        '?', '？',  # Adding full-width question mark (half-width already included)
        '!', '！',  # Adding full-width exclamation mark (half-width already included)
    }

    for segment in transcription_segments:
        # Initialize variables for creating new segments based on punctuation
        new_segment_start = segment['start']
        new_segment_words = []

        for word in segment['words']:
            new_segment_words.append(word)
            # If the word ends with a punctuation mark, conclude the current segment
            if any(word['word'].endswith(punct) for punct in punctuation_marks):
                # Calculate end time of the current segment based on the word's end time
                new_segment_end = word['end']
                # Append the new segment
                refined_segments.append({
                    'start': new_segment_start,
                    'end': new_segment_end,
                    'lang': segment['lang'],
                    'text': ''.join(w['word'] for w in new_segment_words).lstrip(" "),
                    'words': new_segment_words
                })
                # Reset variables for the next segment
                new_segment_start = new_segment_end
                new_segment_words = []

        # Ensure any remaining words form a final segment
        if new_segment_words:
            refined_segments.append({
                'start': new_segment_start,
                'end': segment['end'],
                'lang': segment['lang'],
                'text': ''.join(w['word'] for w in new_segment_words).lstrip(" "),
                'words': new_segment_words
            })

    return refined_segments


# def refine_transcription_segments_with_vad(transcription_segments, vad_segments, sampling_rate):
#     """
#     Refines transcription segments based on VAD segments, dividing using word-level timestamps.

#     Parameters:
#     - transcription_segments: List of transcription segments with word-level detail.
#     - vad_segments: Speech segments detected by VAD.
#     - sampling_rate: The sampling rate of the audio file.

#     Returns:
#     - List of refined transcription segments.
#     """
#     refined_segments = []

#     for trans_seg in transcription_segments:
#         trans_start = int(trans_seg['start'] * sampling_rate)
#         trans_end = int(trans_seg['end'] * sampling_rate)
#         trans_words = trans_seg.get('words', [])

#         overlapping_vad_segments = [vad for vad in vad_segments if not (vad['end'] < trans_start or vad['start'] > trans_end)]

#         if not overlapping_vad_segments:
#             # If no overlapping VAD segments, add the transcription segment as is
#             trans_seg.pop("words")
#             refined_segments.append(trans_seg)
#         else:
#             # Divide the transcription segment based on overlapping VAD segments
#             for vad_seg in overlapping_vad_segments:
#                 vad_start = vad_seg['start']
#                 vad_end = vad_seg['end']
#                 # Filter words that fall within the current VAD segment
#                 words_in_vad = [word for word in trans_words if vad_start <= int(word['start'] * sampling_rate) <= vad_end]
#                 if words_in_vad:
#                     # Create a new segment for each group of words in the VAD segment
#                     segment_text = ''.join(word['word'] for word in words_in_vad).lstrip(" ")
#                     refined_segments.append({
#                         'start': words_in_vad[0]['start'],
#                         'end': words_in_vad[-1]['end'],
#                         'lang': trans_seg['lang'],
#                         'text': segment_text,
#                         'words': words_in_vad  # Include word-level timestamps for further use
#                     })

#     return refined_segments

def refine_segment_with_vad(transcription_segment, vad_segments, sample_rate):
    refined_segments = []

    # Preparation: Convert VAD times to samples if needed
    vad_times = [(vad['start'], vad['end']) for vad in vad_segments]

    current_start = transcription_segment['start']
    segment_end = transcription_segment['end']
    segment_words = transcription_segment.get('words', [])
    
    for word in segment_words:
        word_start = word['start']
        word_end = word['end']
        
        # Check if word overlaps with any VAD segment
        for vad_start, vad_end in vad_times:
            if word_start < vad_end and word_end > vad_start:  # If the word is within a VAD segment
                # Append word to the current segment's text
                if not refined_segments or refined_segments[-1]['end'] < word_start:
                    # Start a new segment if there's no overlap with the last refined segment
                    refined_segments.append({
                        'start': word_start,
                        'end': word_end,
                        'lang': transcription_segment['lang'],
                        'text': word['word'].lstrip(),
                        'words': [word]  # Including the word object itself for reference
                    })
                else:
                    # Otherwise, extend the current segment
                    refined_segments[-1]['end'] = max(refined_segments[-1]['end'], word_end)
                    refined_segments[-1]['text'] += "" + word['word']
                    refined_segments[-1]['words'].append(word)
                break

    # Ensure segment continuity and no loss of information
    if refined_segments:
        # Adjust the first and last segment to match the original boundaries if needed
        refined_segments[0]['start'] = min(refined_segments[0]['start'], current_start)
        refined_segments[-1]['end'] = max(refined_segments[-1]['end'], segment_end)

    return refined_segments


def refine_transcription_segments_with_vad(transcription_segments, vad_segments, sample_rate):
    """
    Refines all transcription segments based on VAD segments.

    Parameters:
    - transcription_segments: List of transcription segments with word-level detail.
    - vad_segments: Speech segments detected by VAD.
    - sample_rate: The sampling rate of the audio file.

    Returns:
    - List of refined transcription segments.
    """
    all_refined_segments = []

    for trans_seg in transcription_segments:
        refined_segments = refine_segment_with_vad(trans_seg, vad_segments, sample_rate)
        all_refined_segments.extend(refined_segments)
    
    # Further processing to assign actual text to each refined segment based on word timings
    # This step is left as an exercise since it requires accessing and dividing the 'text' and 'words'
    # attributes of the original transcription_segments based on the refined segment timings.

    return all_refined_segments


def split_long_segments_by_time(segments, max_duration_seconds=4.0, sample_rate=16000):
    """
    Split segments that are longer than max_duration_seconds.
    
    Parameters:
    - segments: List of subtitle segments
    - max_duration_seconds: Maximum duration for each segment in seconds
    - sample_rate: Audio sample rate
    
    Returns:
    - List of segments with long ones split
    """
    refined_segments = []
    
    for segment in segments:
        duration = (segment['end'] - segment['start']) / sample_rate
        
        if duration <= max_duration_seconds:
            refined_segments.append(segment)
        else:
            # Split long segment by time
            words = segment.get('words', [])
            if not words:
                # If no word-level timestamps, just split in half
                mid_point = (segment['start'] + segment['end']) // 2
                refined_segments.extend([
                    {
                        'start': segment['start'],
                        'end': mid_point,
                        'lang': segment['lang'],
                        'text': segment['text'][:len(segment['text'])//2].strip(),
                    },
                    {
                        'start': mid_point,
                        'end': segment['end'],
                        'lang': segment['lang'],
                        'text': segment['text'][len(segment['text'])//2:].strip(),
                    }
                ])
            else:
                # Split based on words and time
                target_split_time = segment['start'] + (max_duration_seconds * sample_rate)
                split_index = 0
                
                for i, word in enumerate(words):
                    if word['end'] >= target_split_time:
                        split_index = max(1, i)  # Ensure at least one word per segment
                        break
                else:
                    split_index = len(words) // 2  # Fallback to middle
                
                if split_index > 0 and split_index < len(words):
                    first_segment = {
                        'start': segment['start'],
                        'end': words[split_index-1]['end'],
                        'lang': segment['lang'],
                        'text': ''.join(w['word'] for w in words[:split_index]).strip(),
                        'words': words[:split_index]
                    }
                    
                    second_segment = {
                        'start': words[split_index]['start'],
                        'end': segment['end'],
                        'lang': segment['lang'],
                        'text': ''.join(w['word'] for w in words[split_index:]).strip(),
                        'words': words[split_index:]
                    }
                    
                    # Recursively split if still too long
                    refined_segments.extend(split_long_segments_by_time([first_segment], max_duration_seconds, sample_rate))
                    refined_segments.extend(split_long_segments_by_time([second_segment], max_duration_seconds, sample_rate))
                else:
                    refined_segments.append(segment)
    
    return refined_segments


def split_long_segments_by_length(segments, max_chars=60, max_words=10):
    """
    Split segments that are too long by character count or word count.
    
    Parameters:
    - segments: List of subtitle segments
    - max_chars: Maximum characters per segment
    - max_words: Maximum words per segment
    
    Returns:
    - List of segments with long ones split
    """
    refined_segments = []
    
    for segment in segments:
        text = segment['text'].strip()
        words = segment.get('words', [])
        
        # Check if segment is too long
        if len(text) <= max_chars and len(text.split()) <= max_words:
            refined_segments.append(segment)
            continue
        
        # Split long segment
        if words and len(words) > max_words:
            # Split by word count using word timestamps
            mid_point = min(max_words, len(words) // 2)
            
            first_segment = {
                'start': segment['start'],
                'end': words[mid_point-1]['end'],
                'lang': segment['lang'],
                'text': ''.join(w['word'] for w in words[:mid_point]).strip(),
                'words': words[:mid_point]
            }
            
            second_segment = {
                'start': words[mid_point]['start'],
                'end': segment['end'],
                'lang': segment['lang'],
                'text': ''.join(w['word'] for w in words[mid_point:]).strip(),
                'words': words[mid_point:]
            }
            
            # Recursively process split segments
            refined_segments.extend(split_long_segments_by_length([first_segment], max_chars, max_words))
            refined_segments.extend(split_long_segments_by_length([second_segment], max_chars, max_words))
        else:
            # Split by character count (fallback when no word timestamps)
            mid_char = min(max_chars, len(text) // 2)
            # Try to split at a space near the middle
            split_pos = text.rfind(' ', 0, mid_char)
            if split_pos == -1:
                split_pos = mid_char
            
            refined_segments.extend([
                {
                    'start': segment['start'],
                    'end': (segment['start'] + segment['end']) // 2,
                    'lang': segment['lang'],
                    'text': text[:split_pos].strip(),
                },
                {
                    'start': (segment['start'] + segment['end']) // 2,
                    'end': segment['end'],
                    'lang': segment['lang'],
                    'text': text[split_pos:].strip(),
                }
            ])
    
    return refined_segments


def enhanced_punctuation_split(segments):
    """
    Enhanced punctuation splitting with more break opportunities.
    
    Parameters:
    - segments: List of subtitle segments
    
    Returns:
    - List of segments split on more punctuation marks
    """
    refined_segments = []
    
    # Extended punctuation marks including pauses and connectors
    extended_punctuation = {
        '、', ',', '，',  # Commas
        '.', '。',        # Periods
        '?', '？',        # Questions
        '!', '！',        # Exclamations
        ';', '；',        # Semicolons
        ':', '：',        # Colons
        '—', '–', '-',    # Dashes
        '…',              # Ellipsis
    }
    
    # Common pause words/phrases that could indicate natural breaks
    pause_words = {
        'en': [' and ', ' but ', ' so ', ' then ', ' however ', ' therefore '],
        'zh': ['然后', '但是', '所以', '因为', '而且', '或者'],
        'ja': ['そして', 'でも', 'だから', 'それで', 'しかし'],
    }
    
    for segment in segments:
        words = segment.get('words', [])
        if not words:
            refined_segments.append(segment)
            continue
            
        current_segment_start = segment['start']
        current_words = []
        
        for i, word in enumerate(words):
            current_words.append(word)
            
            # Check for punctuation in word
            has_punctuation = any(punct in word['word'] for punct in extended_punctuation)
            
            # Check for pause words
            has_pause_word = False
            current_text = ''.join(w['word'] for w in current_words)
            for lang, pause_list in pause_words.items():
                if any(pause in current_text.lower() for pause in pause_list):
                    has_pause_word = True
                    break
            
            # Split if punctuation found or at pause words, but ensure minimum length
            if (has_punctuation or has_pause_word) and len(current_words) >= 3:
                refined_segments.append({
                    'start': current_segment_start,
                    'end': word['end'],
                    'lang': segment['lang'],
                    'text': ''.join(w['word'] for w in current_words).strip(),
                    'words': current_words.copy()
                })
                
                # Reset for next segment
                current_segment_start = word['end']
                current_words = []
        
        # Add remaining words as final segment
        if current_words:
            refined_segments.append({
                'start': current_segment_start,
                'end': segment['end'],
                'lang': segment['lang'],
                'text': ''.join(w['word'] for w in current_words).strip(),
                'words': current_words
            })
    
    return refined_segments


def reduce_subtitle_length(segments, sample_rate=16000, max_duration=4.0, max_chars=60, max_words=10):
    """
    Main function to reduce subtitle length using multiple methods.
    
    Parameters:
    - segments: List of subtitle segments
    - sample_rate: Audio sample rate
    - max_duration: Maximum duration per segment in seconds
    - max_chars: Maximum characters per segment
    - max_words: Maximum words per segment
    
    Returns:
    - List of shortened segments
    """
    # Step 1: Enhanced punctuation splitting
    segments = enhanced_punctuation_split(segments)
    
    # Step 2: Split by time duration
    segments = split_long_segments_by_time(segments, max_duration, sample_rate)
    
    # Step 3: Split by length (character/word count)
    segments = split_long_segments_by_length(segments, max_chars, max_words)
    
    return segments


def process_audio_segments(speech_timestamps, wav, sampling_rate, model_name, whisper_model, detector, vad_timestamps=None):
    """
    Process audio segments to obtain language and transcription for each segment.

    Parameters:
    - speech_timestamps: List of dictionaries indicating speech segments with 'start' and 'end' frames.
    - wav: The waveform of the audio file.
    - sampling_rate: The sampling rate of the audio file.
    - whisper_model: The Whisper model loaded for transcription.
    - detector: The language detector for identifying the language of a segment.

    Returns:
    - List of dictionaries with transcription and language detection for each segment.
    """

    print("Transcribing audio...")

    

    # init run to obtain the language of each segment
    transcription_timestamps_with_lang = []

    for segment in tqdm(speech_timestamps):
        start_frame, end_frame = segment['start'], segment['end']
        start_sample, end_sample = int(start_frame), int(end_frame)
        # segment_audio = whisper.pad_or_trim(wav[start_sample:end_sample])
        segment_audio = wav[start_sample:end_sample]

        # print("start_frame: ", start_frame)
        # print("end_frame: ", end_frame)
        # print("sampling_rate: ", sampling_rate)
        
        detected_language = predict_language_for_segment(segment_audio, model=model_name)  # Initial language detection
        transcription, words_segments, _ = transcribe_segment(segment_audio, start_frame, end_frame, sampling_rate, detected_language)
        
        # Update segments with detailed language detection and adjust frames
        new_segments = update_segments_with_language(
            words_segments, 
            start_frame, 
            sampling_rate, 
            detector,
            detected_language=detected_language
        )
        transcription_timestamps_with_lang.extend(new_segments)

        # print("start_frame: ", start_frame)
        # print("end_frame: ", end_frame)
        # print("sampling_rate: ", sampling_rate)

        print(f"Segment {start_frame/sampling_rate}-{end_frame/sampling_rate}s (Language: {detected_language}): {transcription}")



    if vad_timestamps:
        # Assuming you have a list `transcription_segments` populated and `sample_rate` defined
        refined_segments = refine_transcription_segments_with_punctuation(transcription_timestamps_with_lang, sampling_rate)

        # Call the function to refine based on VAD
        refined_segments = refine_transcription_segments_with_vad(refined_segments, vad_timestamps, sampling_rate)

        # refined_segments = transcription_timestamps_with_lang

    else:

        refined_segments = transcription_timestamps_with_lang


    # return transcription_timestamps_with_lang
    return filter_segments(refined_segments)


def merge_segments(transcription_timestamps, sample_rate):
    print("Merging segments...")


    # merged based on the language detection from the result of whisper and langua
    merged_segments = []
    current_segment = None
    current_duration = 0



    for segment in tqdm(transcription_timestamps):
        start_frame, end_frame = segment['start'], segment['end']
        segment_duration = (end_frame - start_frame) / sample_rate
        
        if current_segment is None:
            # Initialize the first segment
            current_segment = segment
            current_duration = segment_duration
        # elif (current_segment['lang'] == segment['lang']) and (current_duration + segment_duration <= 30):
        elif (current_segment['lang'] == segment['lang']):
            # Merge segments: extend duration and update end frame
            current_segment['end'] = end_frame
            current_duration += segment_duration
        else:
            # Current segment does not match or exceeds 30 seconds, start a new segment
            merged_segments.append(current_segment)
            current_segment = segment
            current_duration = segment_duration

    # Don't forget to add the last segment
    if current_segment:
        merged_segments.append(current_segment)

    return merged_segments



# def extract_audio_from_video(video_path, output_audio_path):
#     """
#     Extracts the audio from a video file and saves it as a WAV file.

#     Parameters:
#     - video_path: Path to the input video file.
#     - output_audio_path: Path where the extracted audio WAV file should be saved.
#     """
#     # Construct the ffmpeg command to extract audio
#     command = [
#         'ffmpeg', 
#         '-y',  # Overwrite output file if it exists
#         '-i', video_path,  # Input video file
#         '-vn',  # No video
#         '-acodec', 'pcm_s16le',  # WAV audio codec
#         '-ar', '44100',  # Set audio sampling rate to 16000 Hz
#         '-ac', '1',  # Set audio channels to mono
#         output_audio_path  # Output audio file
#     ]

#     # Execute the command
#     subprocess.run(command, check=True)
#     # Execute the command with stdout and stderr muted
#     # subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

# def extract_audio_from_video(video_path, output_audio_path):
#     """
#     Tries to extract audio from a video file. If an exception occurs (likely due to file corruption),
#     it repairs the video outside the monitored folder and retries the extraction process.
    
#     Parameters:
#     - video_path: Path to the input video file.
#     - output_audio_path: Path where the extracted audio WAV file should be saved.
#     """
#     def run_ffmpeg_extract_audio(video_path, output_audio_path):
#         command = [
#             'ffmpeg',
#             '-y',  # Overwrite output file if it exists
#             '-i', video_path,  # Input video file
#             '-vn',  # No video
#             '-acodec', 'pcm_s16le',  # WAV audio codec
#             '-ar', '44100',  # Set audio sampling rate to 44100 Hz
#             '-ac', '1',  # Set audio channels to mono
#             output_audio_path  # Output audio file
#         ]
#         subprocess.run(command, check=True)

#     def repair_video(video_path):
#         # Use a temporary directory to repair the video
#         with tempfile.TemporaryDirectory() as temp_dir:
#             temp_video_path = os.path.join(temp_dir, os.path.basename(video_path))
#             # Attempt to repair the video by re-muxing to a temporary file
#             command = [
#                 'ffmpeg',
#                 '-y',  # Overwrite output file if it exists
#                 '-i', video_path,  # Input video file
#                 '-c', 'copy',  # Copy all streams without re-encoding
#                 '-movflags', '+faststart',  # Move moov atom to the beginning
#                 temp_video_path  # Temporary output video file
#             ]
#             subprocess.run(command, check=True)
#             # Replace the original file with the repaired file
#             shutil.move(temp_video_path, video_path)

#     try:
#         # First attempt to extract audio normally
#         run_ffmpeg_extract_audio(video_path, output_audio_path)
#     except subprocess.CalledProcessError:
#         print(f"Initial audio extraction failed, attempting to repair: {video_path}")
#         # If an exception occurs, repair the video outside the monitored folder
#         repair_video(video_path)
#         # Retry the audio extraction with the repaired video file
#         print("Retrying audio extraction after repair.")
#         run_ffmpeg_extract_audio(video_path, output_audio_path)
#         print(f"Audio extraction successful after repair: {output_audio_path}")


def extract_audio_from_video(video_path, output_audio_path):
    """
    Extracts audio from a video file, enhances the audio for better transcription by increasing its volume,
    and saves it to a specified output path in WAV format. If an exception occurs, likely due to file corruption,
    it repairs the video outside the monitored folder and retries the extraction process.

    Parameters:
    - video_path: Path to the input video file.
    - output_audio_path: Path where the extracted audio WAV file should be saved.
    """

    def run_ffmpeg_extract_audio(video_path, output_audio_path):
        # Command to extract and enhance audio
        command = [
            'ffmpeg',
            '-y',  # Overwrite output file if it exists
            '-i', video_path,  # Input video file
            '-vn',  # No video
            '-af', 'dynaudnorm=f=100',  # Dynamic audio normalization
            '-acodec', 'pcm_s16le',  # WAV audio codec
            '-ar', '44100',  # Set audio sampling rate to 44100 Hz
            '-ac', '1',  # Set audio channels to mono
            output_audio_path  # Output audio file
        ]
        subprocess.run(command, check=True)

    def repair_video(video_path):
        # Use a temporary directory to repair the video
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_video_path = os.path.join(temp_dir, os.path.basename(video_path))
            # Attempt to repair the video by re-muxing to a temporary file
            command = [
                'ffmpeg',
                '-y',  # Overwrite output file if it exists
                '-i', video_path,  # Input video file
                '-c', 'copy',  # Copy all streams without re-encoding
                '-movflags', '+faststart',  # Move moov atom to the beginning
                temp_video_path  # Temporary output video file
            ]
            subprocess.run(command, check=True)
            # Replace the original file with the repaired file
            shutil.move(temp_video_path, video_path)

    try:
        # First attempt to extract and enhance audio normally
        run_ffmpeg_extract_audio(video_path, output_audio_path)
    except subprocess.CalledProcessError:
        print(f"Initial audio extraction failed, attempting to repair: {video_path}")
        # If an exception occurs, repair the video outside the monitored folder
        repair_video(video_path)
        # Retry the audio extraction with the repaired video file
        print("Retrying audio extraction after repair.")
        run_ffmpeg_extract_audio(video_path, output_audio_path)
        print(f"Audio extraction successful after repair: {output_audio_path}")

def clean_subtitles_dict(subtitles):
    """
    Cleans up subtitles by removing specific unwanted lines in Chinese or English.

    :param subtitles: List of subtitle dictionaries with keys 'start', 'end', 'lang', 'text'
    :return: A cleaned list of subtitle dictionaries
    """
    # Lines to be removed if detected
    unwanted_lines = [
        "优优独播",
        "优优独播剧场",
        "YoYo Television",
        "YoYo Television Series Exclusive",
    ]

    # Function to check if the subtitle text matches any unwanted lines
    def is_unwanted_line(text):
        for line in unwanted_lines:
            if line in text:
                return True
        return False

    # Filter out subtitles with unwanted text
    cleaned_subtitles = [subtitle for subtitle in subtitles if not is_unwanted_line(subtitle['text'])]

    return cleaned_subtitles

if __name__ == "__main__": 

    parser = argparse.ArgumentParser(description="Generate subtitles from a video file.")
    # parser.add_argument("video_path", help="Path to the video file.")
    parser.add_argument('-t', '--video-path', required=True, help="Path to the video file.")
    parser.add_argument("--whisper-model", default="large", help="Whisper model to use (default: large).")
    parser.add_argument("--force", action='store_true', help="Force overwrite of existing audio and subtitle files.")

    args = parser.parse_args()

    video_path = args.video_path
    model_name = args.whisper_model
    force = args.force

    # Determine the output audio path based on the input video path
    base_path, _ = os.path.splitext(video_path)
    audio_path = f"{base_path}.wav"



    # Change the SRT and JSON path based on the input audio/video path
    base_path, _ = os.path.splitext(audio_path)
    srt_path = f"{base_path}.srt"
    json_path = f"{base_path}.json"

    # Specify the sampling rate
    sampling_rate = 16000  # Hz

    if not (os.path.exists(srt_path) or os.path.exists(json_path)) or args.force:
        

        try:
            # Initialize the Lingua language detector with specified languages
            languages = [Language.ENGLISH, Language.CHINESE, Language.JAPANESE, Language.ARABIC]  # Adjust languages as needed
            detector = LanguageDetectorBuilder.from_languages(*languages)\
                .with_minimum_relative_distance(0.9)\
                .build()


            # Set the number of threads for PyTorch
            torch.set_num_threads(1)

            # Load the Silero VAD model
            model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False)
            # model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)
            (get_speech_timestamps, _, read_audio, *_) = utils
            # Load Whisper model for language detection and transcription
            whisper_model = whisper.load_model(model_name)

            

            # audio_path = '/home/lachlan/Projects/whisper_with_lang_detect/IMG_6276.wav'
            # video_path = '/home/lachlan/Projects/whisper_with_lang_detect/IMG_6276.MOV'

            
            # Extract audio from the video
            extract_audio_from_video(video_path, audio_path)
            
            

            # Load your audio file
            wav = read_audio(audio_path, sampling_rate=sampling_rate)

            # Get speech timestamps from the audio file using Silero VAD
            speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=sampling_rate)
            audio_length = len(wav)

            adjust_timestamps(speech_timestamps, audio_length)

            # init run to obtain the language of each segment
            transcription_timestamps_with_lang = process_audio_segments(speech_timestamps, wav, sampling_rate, model_name, whisper_model, detector)
            clean_timestamps(transcription_timestamps_with_lang, audio_length)

            # merged based on the language detection from the result of whisper and langua
            merged_segments = merge_segments(transcription_timestamps_with_lang, sampling_rate)

            # print("Transcribe merged VAD...")
            adjust_timestamps(merged_segments, audio_length)


            # second transcription of the language-specified and merged segments
            final_subtitles = process_audio_segments(merged_segments, wav, sampling_rate, model_name, whisper_model, detector, vad_timestamps=speech_timestamps)
            clean_timestamps(final_subtitles, audio_length)

            # Reduce subtitle line length - NEW ADDITION
            final_subtitles = reduce_subtitle_length(
                final_subtitles, 
                sample_rate=sampling_rate, 
                max_duration=4.0,    # Max 4 seconds per line
                max_chars=60,        # Max 60 characters per line  
                max_words=10         # Max 10 words per line
            )

        except Exception as e:
            print("error: ", str(e))
            traceback.print_exc()
            final_subtitles = []

        print("Final subtitles: ")
        for line in final_subtitles:
            print(line)

        # convert the final subtitles into a real subtitles

        # srt_path = "subtitles.srt"  # Specify the save path for the SRT file
        # json_path = "subtitles.json"  # Specify the save path for the JSON file

        

        # Generate SRT and JSON content
        srt_content = subtitles_to_srt(final_subtitles, sampling_rate)
        json_content = subtitles_to_json(final_subtitles, sampling_rate)

        # Save the subtitles
        save_subtitles(srt_content, json_content, srt_path, json_path)


        

        # generator = SubtitleGenerator(whisper_model=args.whisper_model, force=args.force)
        # generator.process_video(args.video_path)