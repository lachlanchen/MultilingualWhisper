<p align="center">
  <img src="https://raw.githubusercontent.com/lachlanchen/lachlanchen/main/logos/banner.png" alt="LazyingArt banner" />
</p>

# MultilingualWhisper

A dropâ€‘in subtitle generator built on OpenAI Whisper, extended with precise perâ€‘segment language detection and refinementâ€”perfect for videos containing multiple languages.

For a stepâ€‘byâ€‘step walkâ€‘through of the pipeline and the key functions, see `SCRIPT_LOGIC.md`.

---

## ğŸš€ Key Features

- **Silero VAD** â†’ Whisper pipeline  
  Voice Activity Detection (VAD) splits audio into speech segments, then Whisper transcribes each chunk.

- **Fineâ€‘grained language detection**  
  Uses [Lingua](https://github.com/pemistahl/linguaâ€‘java) alongside Whisperâ€™s own detector to tag every segment (even individual words) with ISO language codes (en, zh, ja, ar, yue, ko, vi, es, fr, â€¦).

- **Intelligent segment refinement**  
  - **Timestamp cleanup** ensures no gaps or overlaps  
  - **Punctuation splits** break long transcriptions at commas, periods, question marks, etc.  
  - **VAD merges** reâ€‘align words back to VAD blocks for smoother subtitles  

- **Multilingual subtitles**  
  Outputs both `.srt` and `.json`, preserving language tags per segment so you can style or filter by language in downstream players or editors.

- **Robust video support**  
  Autoâ€‘extracts & normalizes audio via FFmpeg, repairs corrupted containers, and normalizes volume for clearer transcripts.

---

## ğŸ”§ Installation

1. **Clone this repo**  
   ```bash
   git clone git@github.com:lachlanchen/MultilingualWhisper.git
   cd MultilingualWhisper
   ```

2. **Create & activate** a virtual environment  
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```

3. **Install dependencies**  
   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ›  Usage

```bash
python vad_lang_subtitle.py \
  --video-path path/to/video.mp4 \
  --whisper-model large \
  [--force]
```

- `--video-path` (`-t`): input video file  
- `--whisper-model`: Whisper variant (tinyâ†’large)  
- `--force`: reâ€‘run even if `.wav`, `.srt`, or `.json` already exist  

After running youâ€™ll get:

- `yourvideo.wav` (enhanced audio)  
- `yourvideo.srt` (timestamped subtitles)  
- `yourvideo.json` (rich JSON with `start`, `end`, `lang`, `text`, and wordâ€‘level timestamps)

---

## ğŸ”Œ LazyEdit Integration

This repo is also used as a submodule in LazyEdit. LazyEdit resolves the script path relative to the repo, so it runs the local copy at `whisper_with_lang_detect/vad_lang_subtitle.py`.

Optional LazyEdit env overrides:

```
LAZYEDIT_WHISPER_SCRIPT=/path/to/LazyEdit/whisper_with_lang_detect/vad_lang_subtitle.py
LAZYEDIT_WHISPER_MODEL=large-v3
LAZYEDIT_WHISPER_FALLBACK_MODEL=large-v2
```

---

## ğŸ“‚ Project Layout

```
.
â”œâ”€â”€ vad_lang_subtitle.py      # Main pipeline: VAD â†’ Whisper â†’ Lingua â†’ refine â†’ save
â”œâ”€â”€ vad_lang_subtitle.py.old  # Legacy prototype
â”œâ”€â”€ data/                     # Optional test media
â”œâ”€â”€ archived/                 # Old experiments
â”œâ”€â”€ vad_lang_subtitle.srt     # Example output
â”œâ”€â”€ vad_lang_subtitle.json    # Example JSON
â””â”€â”€ requirements.txt          # Python deps (whisper, torchaudio, lingua, sileroâ€‘vad, tqdm, etc.)
```

---

## ğŸ”— Acknowledgments

- [OpenAI Whisper](https://github.com/openai/whisper) for speechâ€‘toâ€‘text  
- [Snakers4/Sileroâ€‘VAD](https://github.com/snakers4/sileroâ€‘models) for robust voice activity detection  
- [Lingua](https://github.com/pemistahl/linguaâ€‘java) for highâ€‘accuracy language identification  

---

## ğŸ¤ Contributing

1. Fork & clone  
2. Create a branch: `git checkout -b feat/yourâ€‘idea`  
3. Commit & push  
4. Open a PRâ€”letâ€™s make subtitles smarter!

---

## ğŸ“„ License

MIT Â© Lachlan Chen
